{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drscook/m5364_23sp_data_science1/blob/main/pipelines_AirBNB_ipynb_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Free Cloud Computing \n",
        "## It's not just Ï€ in the sky\n",
        "## April 19, 2023\n",
        "## Tarleton State University\n",
        "## Educational Excellence Week\n",
        "### Dr. Scott Cook\n",
        "### Chief Data Scientist & Assoc Prof of Mathematics"
      ],
      "metadata": {
        "id": "8JB4_fd4xCUU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXCkHriQbe1R"
      },
      "outputs": [],
      "source": [
        "# upgrade to most recent versions (Pandas 2.0 released on April 6, 2023)\n",
        "# run once at start of session\n",
        "# ignore any \"pip dependency\" errors\n",
        "! pip install -q --upgrade numpy pandas scikit-learn matplotlib shap jupyter-autotime\n",
        "# restart kernel so upgrade takes effect\n",
        "from IPython import get_ipython\n",
        "get_ipython().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Major Scientific Python Packages\n",
        "1. Numpy - https://numpy.org/ - Foundational scientific computing package (arrays)\n",
        "1. Pandas - https://pandas.pydata.org/ - Most important data science package (dataframes)\n",
        "1. Scikit-Learn - https://scikit-learn.org - Machine Learning\n",
        "1. Jupyter Notebook - https://jupyter.org/ - Front end user interface you are using now\n",
        "1. Google Colab - https://colab.research.google.com/ - Cloud compute platform you are using now\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BeQLmb_iUck_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do a little machine learning project using the Boston Airbnb dataset from OpenML.org https://www.openml.org/search?type=data&status=active&id=43819"
      ],
      "metadata": {
        "id": "x7xFLMCvpJpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "airbnb = fetch_openml(data_id=43819, parser=\"auto\")\n",
        "airbnb.data.head(3)"
      ],
      "metadata": {
        "id": "zo9WhAQV8L2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could directly use as is. But, most projects use data stored in a local file, like a csv. So, to demonstrate how to read a csv, we will write to a csv and then read it back in.\n",
        "\n",
        "Warning: Colab borrows a machine from Google for you to use.  But Google wants it back when you are done. This Jupyter notebook automatically saves to your Google Drive (folder \"Colab Notebook\"). But any other file you upload or create will be deleted unless you mount and save to your google drive.\n",
        "\n",
        "There are 2 ways to mount Drive.\n",
        "1. Try cell below\n",
        "1. If it errs, click the folder icon on far left under {x}, then click icon of folder with triangle (says Mount Drive when you hover over it)"
      ],
      "metadata": {
        "id": "YzTj8MscVak-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, google.colab, pandas as pd\n",
        "root = pathlib.Path(\"/content/drive\")\n",
        "google.colab.drive.mount(str(root))"
      ],
      "metadata": {
        "id": "m4t7zGq_Qb68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell below \n",
        "1. creates a folder named \"CEE_workshop\" in drive/MyDrive\n",
        "1. saves data to a csv file\n",
        "1. reads data back in"
      ],
      "metadata": {
        "id": "zF5FIhPY2kKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = root / \"MyDrive/CEE_workshop\"\n",
        "path.mkdir(exist_ok=True)\n",
        "file = path / \"airbnb.csv\"\n",
        "airbnb.data.to_csv(file, index=False)  # saves to your drive\n",
        "df = pd.read_csv(file) # reads that file back in"
      ],
      "metadata": {
        "id": "rh7_sMYL_Y9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to read a csv, simply upload it to google drive, edit the path to point at it, and use read_csv. Pandas supports other file formats like excel, parquet, json, pickle, hdf, and more."
      ],
      "metadata": {
        "id": "j6UpBDfIYxWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We typically spend a lot of time doing data exploration and visualization at the start of a project so we can make smart choices about data wrangling and modeling approaches. Python has some fantastic [visualization packages](https://pyviz.org/overviews/index.html) like [Bokeh](https://docs.bokeh.org/), [Seaborn](https://seaborn.pydata.org/), [Plotly](https://plotly.com/python/), [Matplotlib](https://matplotlib.org/), and many others.\n",
        "\n",
        "There are many great tutorials online for that. So, for the sake of time, we'll assume you've completed this step and skip to wrangling and modeling.\n",
        "\n",
        "Again, the goal of this workshop is to show what is possible using Colab.  It is not meant to teach Python. So don't worry about understanding all the code below.\n",
        "\n",
        "Our task: Predict \"review_scores_value\".  This is the overall rating for an airbnb property and combines the other \"review_scores_X\" columns.  So, we can't use those columns as inputs (features) ... that would be cheating."
      ],
      "metadata": {
        "id": "DEy5Vir4giOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIstAtyl2z4v"
      },
      "outputs": [],
      "source": [
        "import pathlib, numpy as np, pandas as pd, sklearn, missingno as msno\n",
        "from sklearn import set_config\n",
        "set_config(transform_output = \"pandas\")\n",
        "\n",
        "df = pd.read_csv(file).convert_dtypes().set_index('id') # convert_dtypes activates new \"nullable\" datatypes in pandas https://pandas.pydata.org/pandas-docs/version/1.1/user_guide/integer_na.html\n",
        "\n",
        "# little helper to easily show the first 3 lines as we progress through data wrangling\n",
        "def disp(X):\n",
        "    display(X.head(3))\n",
        "\n",
        "# force all strings to lower case\n",
        "# Python style guides prefer \"easier to ask forgivenes than permission\" over \"look before you leap\"\n",
        "# Some columns are not strings. We simply attempt to lower case everything and fail gracefully for non-strings.\n",
        "for col in df.columns:\n",
        "    try:\n",
        "        df[col] = df[col].str.lower()\n",
        "    except AttributeError:\n",
        "        pass\n",
        "\n",
        "# price has entries like \"6,000.00\". The comma forces this to be a string rather than float.  Let's fix that\n",
        "df['price'] = df['price'].str.replace(',','').astype(float)\n",
        "disp(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "house_rules is free-form text of the rules of the property. Let's try to extract something useful from it."
      ],
      "metadata": {
        "id": "UJ5kUY6WkxcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in df['house_rules'][100:104]:\n",
        "    print(x)\n",
        "    print()"
      ],
      "metadata": {
        "id": "8zI5Q8ccjvSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to deterimine if pets and smoking are allowed\n",
        "# We guess that pets are allowed unless \"no pets\" appears in house_rules.\n",
        "# This is clearly too crude and will miss other expressions like \"pets are not allowed\".\n",
        "# We can rethink this later if the resulting model does not perform well.\n",
        "df['pets_ok'] = ~df['house_rules'].str.contains('no pets')\n",
        "\n",
        "# During data exploration, we observed anytime smoking was discussed in house_rules, it was prohibited.\n",
        "# In other words, we did not see explicit permission to smoke.\n",
        "# We will make a crude assumption that smoking is prohibited anytime \"smok\" appears in house_rules\n",
        "# We can rethink this later if the resulting model does not perform well.\n",
        "df['smoking_ok'] = ~df['house_rules'].str.contains('smok')\n",
        "\n",
        "# If we wanted to be more careful, we could search for multiple expressions like this:\n",
        "# df['smoking_ok'] = ~df['house_rules'].str.contains('|'.join(['smoking', 'non smoking', 'non-smoking', 'smoking not permitted', 'smoking is not permitted']))\n",
        "# https://stackoverflow.com/questions/48541444/pandas-filtering-for-multiple-substrings-in-series/48600345#48600345"
      ],
      "metadata": {
        "id": "50dt7w7Ij8hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "771gPJTQ_qtT"
      },
      "outputs": [],
      "source": [
        "# Look for missing data\n",
        "msno.bar(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlations of missing data among pairs of columns\n",
        "msno.heatmap(df)"
      ],
      "metadata": {
        "id": "0QLNKwbGhgGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIHQ2qhWxEVu"
      },
      "outputs": [],
      "source": [
        "target = 'review_scores_rating'\n",
        "\n",
        "# decide which columns we will use as features (inputs) in the model\n",
        "features = [\n",
        "    'price',\n",
        "    'bedrooms',\n",
        "    'bathrooms',\n",
        "    'cleaning_fee',\n",
        "    'pets_ok',\n",
        "    'smoking_ok',\n",
        "    'amenities_dict',\n",
        "    'neighbourhood_cleansed',\n",
        "]\n",
        "numeric_features = ['price', 'bedrooms', 'bathrooms', 'cleaning_fee', 'pets_ok', 'smoking_ok']  # which features are already numeric\n",
        "\n",
        "mask = df[target].notnull() # find columns where target is NOT missing\n",
        "F = df.loc[mask, features]  # feature dataframe of feature columns and rows where target is not missing\n",
        "y = df.loc[mask, target]    # target dataframe of target column and rows where target is not missing\n",
        "msno.bar(F)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "amenities_dict looks important - it indicates which amenities the property offers.  However, it has been stored in a rather awkward format.  Let's extract it as indicator (dummy) variables."
      ],
      "metadata": {
        "id": "D6-RbRHKtikl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in F['amenities_dict'][:3]:\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "ZuVHpx8C45cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract amenities into indicator columns\n",
        "A = F['amenities_dict'].str.split(', ', expand=True)  # split pieces of amentities string into separate columns\n",
        "disp(A)\n",
        "\n",
        "amenities_features = A.columns = [x.split(':')[0].strip(\"'\") for x in A.iloc[0]]  # get columns names from row 0\n",
        "A.columns = amenities_features\n",
        "disp(A)\n",
        "\n",
        "def amentity_offered(x):\n",
        "    return (x.str[-1] == '1').astype('boolean')  # return True if the final character is 1\n",
        "A = A.apply(amentity_offered)\n",
        "disp(A)"
      ],
      "metadata": {
        "id": "9LRoh7kDt0I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neighborhood might also be important. Most machine learners can't handle categorical variables like this directly, but we can \"one-hot-encode\" into indicator (dummy) variables.  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
      ],
      "metadata": {
        "id": "29WeVyEDuyrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(sparse_output=False)  # create one-hot encoder object\n",
        "N = enc.fit_transform(F[['neighbourhood_cleansed']])#.astype('boolean')  # apply object to neighborhood_cleansed column\n",
        "disp(N)\n",
        "\n",
        "# we don't need the \"neighborhood_cleansed_\" prefix here, so let's remove it by keeping only charaters 23 and beyond\n",
        "neighorhoods_features = N.columns.str[23:]\n",
        "N.columns = neighorhoods_features\n",
        "disp(N)"
      ],
      "metadata": {
        "id": "OxYtAu9LX3V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join and drop the original columns\n",
        "X = F.join(N).join(A).drop(columns=['amenities_dict', 'neighbourhood_cleansed'])\n",
        "disp(X)\n",
        "\n",
        "display(X.describe(include='all'))"
      ],
      "metadata": {
        "id": "egE0Jukgv-BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We are done with data wrangling.  Let's compact this code into one cell for convenience."
      ],
      "metadata": {
        "id": "gUd0P2ZqxIdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autotime\n",
        "import pathlib, google.colab, numpy as np, pandas as pd, sklearn, shap, matplotlib.pyplot as plt, missingno as msno\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import set_config\n",
        "set_config(transform_output = \"pandas\")\n",
        "def disp(X):\n",
        "    display(X.head(3))\n",
        "\n",
        "root = pathlib.Path(\"/content/drive\")\n",
        "google.colab.drive.mount(str(root))\n",
        "path = root / \"MyDrive/CEE_workshop\"\n",
        "path.mkdir(exist_ok=True)\n",
        "file = path / \"airbnb.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file)\n",
        "    print(f'using data saved at {file}')\n",
        "except:\n",
        "    print(f'donwloading data')\n",
        "    airbnb = fetch_openml(data_id=43819, parser=\"auto\")\n",
        "    df = airbnb.data\n",
        "    df.to_csv(file, index=False)\n",
        "df = df.convert_dtypes().set_index('id')\n",
        "\n",
        "for col in df.columns:\n",
        "    try:\n",
        "        df[col] = df[col].str.lower()\n",
        "    except AttributeError:\n",
        "        pass\n",
        "df['price'] = df['price'].str.replace(',','').astype(float)\n",
        "df['pets_ok'] = ~df['house_rules'].str.contains('no pets')\n",
        "df['smoking_ok'] = ~df['house_rules'].str.contains('smok')\n",
        "\n",
        "target = 'review_scores_rating'\n",
        "features         = ['price', 'bedrooms', 'bathrooms', 'cleaning_fee', 'pets_ok', 'smoking_ok', 'amenities_dict', 'neighbourhood_cleansed']\n",
        "numeric_features = ['price', 'bedrooms', 'bathrooms', 'cleaning_fee', 'pets_ok', 'smoking_ok']\n",
        "mask = df[target].notnull()\n",
        "F = df.loc[mask, features]\n",
        "\n",
        "A = df['amenities_dict'].str.split(', ', expand=True)\n",
        "amenities_features = A.columns = [x.split(':')[0].strip(\"'\") for x in A.iloc[0]]\n",
        "A.columns = amenities_features\n",
        "def amentity_offered(x):\n",
        "    return (x.str[-1] == '1').astype('boolean')\n",
        "A = A.apply(amentity_offered)\n",
        "\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "N = enc.fit_transform(df[['neighbourhood_cleansed']]).astype('boolean')\n",
        "neighorhoods_features = N.columns.str[23:]\n",
        "N.columns = neighorhoods_features\n",
        "\n",
        "X = F.join(N).join(A).drop(columns=['amenities_dict', 'neighbourhood_cleansed'])\n",
        "disp(X)"
      ],
      "metadata": {
        "id": "eq-S-yE5F7BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start building a model. I decided not to use neighborhoods after all because there are a lot of them (25) and I'm worried about the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
        "\n",
        "I have the same concern about amenities, so we'll use Principal Components Analysis to reduce their dimension. But I don't think that will work for neighborhoods, so we'll exclude neighborhood for now. Perhaps a future version will include them."
      ],
      "metadata": {
        "id": "-XKTKxk-0_9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "import joblib\n",
        "\n",
        "# create target\n",
        "y = df.loc[mask, target]\n",
        "\n",
        "# set random seed for reproducible results\n",
        "seed = 42\n",
        "\n",
        "# create holdout set to estimate generalization performance after tuning\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.10, random_state=seed)\n",
        "\n",
        "# create preprocessing pipeline\n",
        "# Rescale numerics to [0,1]\n",
        "numeric_prep = Pipeline(steps = [('scaler', MinMaxScaler())])\n",
        "\n",
        "# Principal Components Analysis to reduce dimensionality of 133 amenities \n",
        "amenities_prep = Pipeline(steps = [('pca', PCA())])\n",
        "\n",
        "# combine ouput of numeric & amenities pipelines\n",
        "combined_prep = ColumnTransformer(transformers = [('numeric', numeric_prep, numeric_features), ('amenities', amenities_prep, amenities_features)])\n",
        "\n",
        "# Use K-nearest neighbors imputer to fill missing values\n",
        "prep = Pipeline(steps = [('combined', combined_prep), ('imputer', KNNImputer())])\n",
        "display(prep)\n",
        "\n",
        "# dictionary to store models\n",
        "models = dict()\n",
        "\n",
        "# function to make training with different supervised learners (regressors) convenient\n",
        "def run_model(learner, hyperparameters):\n",
        "    # append final regressor to pipeline\n",
        "    estimator = Pipeline(steps = [('prep', prep), ('learner', learner)])\n",
        "    \n",
        "    # prepare to tune hyperparameters using either exhaustive grid search or randomized grid search\n",
        "    model = RandomizedSearchCV(estimator, hyperparameters, cv=5, scoring='r2')\n",
        "    \n",
        "    # give model a name based on its regessor\n",
        "    model.name = str(learner).split('(')[0]\n",
        "\n",
        "    print(f'Fitting {model.name} with hyperparameter grid:', hyperparameters)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Apply best model to holdout set to estimate generalization performance\n",
        "    model.generalization_score = model.score(X_holdout, y_holdout)\n",
        "    print(f'best model: R^2={model.generalization_score:.3f} with hyperparameters {model.best_params_}')\n",
        "\n",
        "    y_pred = model.predict(X_holdout)\n",
        "    p = [np.min([y_pred, y_holdout]), np.max([y_pred, y_holdout])]\n",
        "    plt.plot(p, p, 'black')\n",
        "    plt.plot(y_holdout, y_pred, '.')\n",
        "\n",
        "    # save file to google drive folder\n",
        "    file = path / f'{model.name}.model'\n",
        "    model.file = file\n",
        "    joblib.dump(model, file)\n",
        "    return model"
      ],
      "metadata": {
        "id": "o3k7WzTrzLoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our complete preprocessing pipeline. We just need one final step - the supervised learning algorithm. Since the target variable is continuous, we are doing a *regression* task.\n",
        "\n",
        "There are many supervised learning algorithms in Scikit-Learn, so we may want to try several of them. Let's write a helper function to make this easy."
      ],
      "metadata": {
        "id": "86uQ2UBIlNGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many of these steps involve tunable hyperparameters where there is not a clear \"best\" option. We want to try many different settings for these hyperparameters to see what performs best. This would take signifcant coding to implement yourself. Luckily, Scikit-Learn offer \"GridSearchCV\" and \"RandomizedSearchCV\" which automate this process. We just give it a dictionary of hyperparameters with values to try and it will loop over them to find the best."
      ],
      "metadata": {
        "id": "Rl0Zoo8tm07g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-nearest neighbors\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "learner = KNeighborsRegressor()\n",
        "hyperparameters = {\n",
        "    'prep__combined__amenities__pca__n_components': [2,4],\n",
        "    'prep__imputer__n_neighbors': [3,7],\n",
        "    'learner__n_neighbors': [3,6,9],\n",
        "}\n",
        "model = run_model(learner, hyperparameters)\n",
        "models[model.name] = model"
      ],
      "metadata": {
        "id": "Ji1QL_B4yPAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machines\n",
        "from sklearn.svm import SVR\n",
        "learner = SVR()\n",
        "hyperparameters = {\n",
        "    'prep__combined__amenities__pca__n_components': [2,4],\n",
        "    'prep__imputer__n_neighbors': [3,7],\n",
        "    'learner__C': np.linspace(0.1,5,3),\n",
        "}\n",
        "model = run_model(learner, hyperparameters)\n",
        "models[model.name] = model"
      ],
      "metadata": {
        "id": "JFilCapMzqnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "learner = RandomForestRegressor(random_state=seed)\n",
        "hyperparameters = {\n",
        "    'prep__combined__amenities__pca__n_components': [2,4],\n",
        "    'prep__imputer__n_neighbors': [3,7],\n",
        "    'learner__max_depth': [3,6,9],\n",
        "}\n",
        "model = run_model(learner, hyperparameters)\n",
        "models[model.name] = model"
      ],
      "metadata": {
        "id": "bjswYiOxzjnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, these models suck. R^2 ranges from 0 (terrible) to 1 (perfect). Even our best model is around 0.2.  So, we want to go back and rethink some choices made along the way. But we built this code so that will probably not require much effort or recoding."
      ],
      "metadata": {
        "id": "vP4qnXUfmhLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, let's bin the target and demonstrate a classification task"
      ],
      "metadata": {
        "id": "alvrv5--wUOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "import joblib\n",
        "\n",
        "y = df.loc[mask, target]\n",
        "# In order to make this a classification task, we will use pandas qcut function to bin the target values into n_labels classes with roughly equal frequencies\n",
        "n_labels = 2\n",
        "y_binned = pd.qcut(df.loc[mask, target], q=n_labels, precision=1)\n",
        "y    = y_binned.cat.codes\n",
        "bins = y_binned.cat.categories.astype(str).to_list()\n",
        "\n",
        "# set random seed for reproducible results\n",
        "seed = 42\n",
        "\n",
        "# create holdout set to estimate generalization performance after tuning\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.10, random_state=seed)\n",
        "\n",
        "# create preprocessing pipeline\n",
        "# Rescale numerics to [0,1]\n",
        "numeric_prep = Pipeline(steps = [('scaler', MinMaxScaler())])\n",
        "\n",
        "# Principal Components Analysis to reduce dimensionality of 133 amenities \n",
        "amenities_prep = Pipeline(steps = [('pca', PCA())])\n",
        "\n",
        "# combine ouput of numeric & amenities pipelines\n",
        "combined_prep = ColumnTransformer(transformers = [('numeric', numeric_prep, numeric_features), ('amenities', amenities_prep, amenities_features)])\n",
        "\n",
        "# Use K-nearest neighbors imputer to fill missing values\n",
        "prep = Pipeline(steps = [('combined', combined_prep), ('imputer', KNNImputer())])\n",
        "display(prep)\n",
        "\n",
        "# dictionary to store models\n",
        "models = dict()\n",
        "\n",
        "# function to make training with different supervised learners (regressors) convenient\n",
        "def run_model(learner, hyperparameters):\n",
        "    # append final regressor to pipeline\n",
        "    estimator = Pipeline(steps = [('prep', prep), ('learner', learner)])\n",
        "    \n",
        "    # prepare to tune hyperparameters using either exhaustive grid search or randomized grid search\n",
        "    model = GridSearchCV(estimator, hyperparameters, cv=5, scoring='f1_micro')\n",
        "    \n",
        "    # give model a name based on its regessor\n",
        "    model.name = str(learner).split('(')[0]\n",
        "\n",
        "    print(f'Fitting {model.name} with hyperparameter grid:', hyperparameters)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Apply best model to holdout set to estimate generalization performance\n",
        "    model.generalization_score = model.score(X_holdout, y_holdout)\n",
        "    print(f'best model: F1={model.generalization_score:.3f} with hyperparameters {model.best_params_}')\n",
        "\n",
        "    print(classification_report(y_holdout, model.predict(X_holdout), target_names=bins))\n",
        "\n",
        "    ConfusionMatrixDisplay.from_estimator(model, X_holdout, y_holdout, display_labels=bins)\n",
        "    plt.show()\n",
        "    \n",
        "    if n_labels == 2:\n",
        "        RocCurveDisplay.from_estimator(model, X_holdout, y_holdout)\n",
        "        plt.plot([0,1],[0,1])\n",
        "        plt.show()\n",
        "\n",
        "    # save file to google drive folder\n",
        "    file = path / f'{model.name}.model'\n",
        "    model.file = file\n",
        "    joblib.dump(model, file)\n",
        "    return model\n",
        "    "
      ],
      "metadata": {
        "id": "oQ47R7cHajki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-nearest neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "learner = KNeighborsClassifier()\n",
        "hyperparameters = {\n",
        "    'prep__combined__amenities__pca__n_components': [2,4],\n",
        "    'prep__imputer__n_neighbors': [3,7],\n",
        "    'learner__n_neighbors': [3,6,9],\n",
        "}\n",
        "model = run_model(learner, hyperparameters)\n",
        "models[model.name] = model"
      ],
      "metadata": {
        "id": "tMTDjqe2cIeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machines\n",
        "from sklearn.svm import SVC\n",
        "learner = SVC()\n",
        "hyperparameters = {\n",
        "    'prep__combined__amenities__pca__n_components': [2,4],\n",
        "    'prep__imputer__n_neighbors': [3,7],\n",
        "    'learner__C': np.linspace(0.1,5,3),\n",
        "}\n",
        "model = run_model(learner, hyperparameters)\n",
        "models[model.name] = model"
      ],
      "metadata": {
        "id": "EEBi833Vw3pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "learner = RandomForestClassifier(random_state=seed)\n",
        "hyperparameters = {\n",
        "    'prep__combined__amenities__pca__n_components': [2,4],\n",
        "    'prep__imputer__n_neighbors': [3,7],\n",
        "    'learner__max_depth': [3,6,9],\n",
        "}\n",
        "model = run_model(learner, hyperparameters)\n",
        "models[model.name] = model"
      ],
      "metadata": {
        "id": "IcbLpg6Pwx1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- How students submit\n",
        "    - Share & send link link (email or canvas)\n",
        "    - Post to Github (not private by default)\n",
        "    - Download & send .ipynb (email or canvas)\n",
        "    - Create & send static html file: https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab\n"
      ],
      "metadata": {
        "id": "3-4ltXJ5zIir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My favorite (free) books are:\n",
        "- [WhirlwindTourOfPython](https://github.com/jakevdp/WhirlwindTourOfPython)\n",
        "- [PythonDataScienceHandbook](https://github.com/jakevdp/PythonDataScienceHandbook)\n",
        "\n",
        "The internet is full of great Python & data science materials. Sadly, they are all out of date (to some extent) before they are published. Code examples often don't work because a package it uses has changed. But that's the price we pay for an amazingly rich ecosystem of free and powerful tools."
      ],
      "metadata": {
        "id": "UCmcAywYNj35"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1r8X4EG8bfgcQORxQaLirFT5eBAKQCPBF",
      "authorship_tag": "ABX9TyOwnAlYYeaR9BBYjlWSCOkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}